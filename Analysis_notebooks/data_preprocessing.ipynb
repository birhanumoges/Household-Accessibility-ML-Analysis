{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095e6e9b",
      "metadata": {
        "id": "095e6e9b"
      },
      "outputs": [],
      "source": [
        "print('Data preprocessing started')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4124644d",
      "metadata": {
        "id": "4124644d"
      },
      "outputs": [],
      "source": [
        "# reading data on google drive\n",
        "from google.colab import drive\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load CSV file from Google Drive\n",
        "file_path = '/content/drive/MyDrive/other/data/Data for MSC Thesis/eth_householdgeovariables_y5.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3969485c",
      "metadata": {
        "id": "3969485c"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# DATA EXPLORATION: NULL AND OUTLIER PERCENTAGE\n",
        "# ------------------------------\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Dataset Overview\n",
        "# ------------------------------\n",
        "print(\"----- Dataset Info -----\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n----- Dataset Shape -----\")\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\n----- Statistical Summary -----\")\n",
        "print(df.describe())\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: Missing Values by Percentage\n",
        "# ------------------------------\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
        "print(\"\\n----- Missing Values Summary -----\")\n",
        "print(missing_df.sort_values(by='Percentage', ascending=False))\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Outlier Detection by Percentage (Numeric Columns Only)\n",
        "# ------------------------------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "outlier_summary = []\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Detect outliers\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
        "    outlier_count = len(outliers)\n",
        "    outlier_percent = (outlier_count / len(df)) * 100\n",
        "\n",
        "    outlier_summary.append([col, outlier_count, outlier_percent])\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary, columns=['Column', 'Outlier Count', 'Outlier %'])\n",
        "print(\"\\n----- Outlier Summary by Percentage -----\")\n",
        "print(outlier_df.sort_values(by='Outlier %', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# FULL PREPROCESSING PIPELINE WITH SMOTE\n",
        "# ------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Drop irrelevant/empty columns\n",
        "# ------------------------------\n",
        "drop_cols = ['household_id'] + [col for col in df.columns if col.startswith('c2_')]\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: Impute missing values\n",
        "# ------------------------------\n",
        "df['lat_dd_mod'].fillna(df['lat_dd_mod'].median(), inplace=True)\n",
        "df['lon_dd_mod'].fillna(df['lon_dd_mod'].median(), inplace=True)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Handle outliers (>5% outliers)\n",
        "# ------------------------------\n",
        "outlier_cols = ['sq4', 'h2021_wetQ', 'dist_road', 'h2021_sen', 'sen_avg']\n",
        "\n",
        "for col in outlier_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df[col] = np.where(df[col] > upper, upper,\n",
        "                       np.where(df[col] < lower, lower, df[col]))\n",
        "\n",
        "# ------------------------------\n",
        "# Step 4: Encode categorical variables\n",
        "# ------------------------------\n",
        "categorical_cols = ['ssa_aez09', 'landcov']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 5: Scale numeric features\n",
        "# ------------------------------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols.remove('suppress')  # exclude target\n",
        "scaler = StandardScaler()\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# ------------------------------\n",
        "# Step 6: Split features and target\n",
        "# ------------------------------\n",
        "X = df.drop('suppress', axis=1)\n",
        "y = df['suppress']\n",
        "\n",
        "# Stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Step 7: Handle class imbalance using SMOTE\n",
        "# ------------------------------\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Preprocessing complete with SMOTE!\")\n",
        "print(f\"Original training set shape: {X_train.shape}, positives: {y_train.sum()}\")\n",
        "print(f\"Resampled training set shape: {X_train_res.shape}, positives: {y_train_res.sum()}\")\n",
        "print(f\"Test set shape: {X_test.shape}, positives: {y_test.sum()}\")\n"
      ],
      "metadata": {
        "id": "xVcHlBG8MyKA"
      },
      "id": "xVcHlBG8MyKA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}